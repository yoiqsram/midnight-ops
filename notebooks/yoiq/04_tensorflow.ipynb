{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "from lib.constants import PROJECT_DIR, EXPERIMENT_NAME, MLFLOW_URI\n",
    "from lib.dataset import load_train_data, load_test_data\n",
    "\n",
    "# Make sure to have the MLFlow server on before running this code.\n",
    "# mlflow.set_tracking_uri(uri=MLFLOW_URI)\n",
    "# experiment = mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "X_train, y_train = load_train_data()\n",
    "X_test = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 189, 3, 259, 245, 343, 329],\n",
       " [6, 16, 4, 30, 330, 1, 2, 260, 162, 379],\n",
       " [4, 16, 13, 21, 68],\n",
       " [5, 14, 1, 38],\n",
       " [4, 30, 1, 53, 119, 2, 75, 55]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lib.sklearn.preprocess import nlp\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "max_words = 750\n",
    "custom_map = {\n",
    "    row['asal']: row['tujuan']\n",
    "    for _, row in pd.read_csv('custom-mapper.csv').iterrows()\n",
    "}\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('tokenizer', nlp.TextTokenizer()),\n",
    "    ('formalizer', nlp.WordsFormalizer()),\n",
    "    ('custom_mapper', nlp.WordsMapper(custom_map)),\n",
    "    ('lemmatization', nlp.WordsLemmatization()),\n",
    "    ('special_char_filter', nlp.SpecialCharacterFilter()),\n",
    "    ('unknown_words_filter', nlp.UnknownWordsFilter()),\n",
    "    ('text_to_sequence', nlp.TokenSequenceTransformer(max_words=max_words))\n",
    "])\n",
    "\n",
    "preprocess_pipeline.fit(X_train + X_test)\n",
    "X_train_transformed = preprocess_pipeline.transform(X_train)\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "X_train_transformed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8679"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_len = 10\n",
    "max_len = 20\n",
    "\n",
    "X_train_seq, y_train_seq = nlp.split_sequences(\n",
    "    X_train_transformed,\n",
    "    y=y_train,\n",
    "    max_len=max_len,\n",
    "    min_len=min_len\n",
    ")\n",
    "X_test_seq = nlp.split_sequences(\n",
    "    X_test_transformed,\n",
    "    max_len=max_len,\n",
    "    min_len=min_len\n",
    ")\n",
    "\n",
    "len(X_train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 21:50:00.525238: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-03-12 21:50:00.527026: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2024-03-12 21:50:02.080091: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-03-12 21:50:02.080155: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (blackbox): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "def f1_macro(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float')\n",
    "    y_pred = K.cast(y_pred, 'float')\n",
    "\n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    fp = K.sum(K.max(K.clip(y_pred - y_true, 0, 1), axis=1), axis=0)\n",
    "    fn = K.sum(K.max(K.clip(y_true - y_pred, 0, 1), axis=1), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2 * p * r / (p + r + K.epsilon())\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_macro_loss(y_true, y_pred):\n",
    "    return 1 - f1_macro(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.sklearn.model.classifier import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, Embedding, LSTM, Maximum\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=32, input_length=max_len),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model_pipeline = KerasClassifier(\n",
    "    model,\n",
    "    optimizer='adam',\n",
    "    loss=f1_macro_loss,\n",
    "    metrics=[f1_macro],\n",
    "    batch_size=64,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "136/136 [==============================] - 4s 17ms/step - loss: 0.8853 - f1_macro: 0.1147\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.7148 - f1_macro: 0.2852\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.5731 - f1_macro: 0.4269\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 2s 18ms/step - loss: 0.4347 - f1_macro: 0.5653\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 3s 18ms/step - loss: 0.3517 - f1_macro: 0.6483\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.2797 - f1_macro: 0.7203\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 2s 18ms/step - loss: 0.2420 - f1_macro: 0.7580\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.2319 - f1_macro: 0.7681\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.2241 - f1_macro: 0.7759\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.1952 - f1_macro: 0.8048\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.1873 - f1_macro: 0.8127\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1842 - f1_macro: 0.8158\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1701 - f1_macro: 0.8299\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1752 - f1_macro: 0.8248\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1698 - f1_macro: 0.8303\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1643 - f1_macro: 0.8357\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1321 - f1_macro: 0.8679\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1245 - f1_macro: 0.8755\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1109 - f1_macro: 0.8891\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1154 - f1_macro: 0.8846\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0995 - f1_macro: 0.9005\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.1103 - f1_macro: 0.8897\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1038 - f1_macro: 0.8962\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0971 - f1_macro: 0.9029\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1017 - f1_macro: 0.8983\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0900 - f1_macro: 0.9100\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0788 - f1_macro: 0.9212\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1279 - f1_macro: 0.8721\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1131 - f1_macro: 0.8869\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1091 - f1_macro: 0.8909\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0983 - f1_macro: 0.9017\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.1111 - f1_macro: 0.8889\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1137 - f1_macro: 0.8863\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1067 - f1_macro: 0.8933\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1095 - f1_macro: 0.8905\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0992 - f1_macro: 0.9008\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0837 - f1_macro: 0.9163\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0938 - f1_macro: 0.9062\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0969 - f1_macro: 0.9030\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0856 - f1_macro: 0.9144\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1050 - f1_macro: 0.8950\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.1208 - f1_macro: 0.8792\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0877 - f1_macro: 0.9123\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0935 - f1_macro: 0.9065\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0917 - f1_macro: 0.9083\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0765 - f1_macro: 0.9235\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0802 - f1_macro: 0.9198\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0808 - f1_macro: 0.9192\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0709 - f1_macro: 0.9291\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0701 - f1_macro: 0.9299\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0698 - f1_macro: 0.9302\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0858 - f1_macro: 0.9142\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0788 - f1_macro: 0.9212\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0956 - f1_macro: 0.9044\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0828 - f1_macro: 0.9172\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0735 - f1_macro: 0.9265\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1095 - f1_macro: 0.8905\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.1027 - f1_macro: 0.8973\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0844 - f1_macro: 0.9156\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0900 - f1_macro: 0.9100\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1113 - f1_macro: 0.8887\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0778 - f1_macro: 0.9222\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0747 - f1_macro: 0.9253\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0677 - f1_macro: 0.9323\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0757 - f1_macro: 0.9243\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0880 - f1_macro: 0.9120\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0899 - f1_macro: 0.9101\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0801 - f1_macro: 0.9199\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0707 - f1_macro: 0.9293\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0770 - f1_macro: 0.9230\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0884 - f1_macro: 0.9116\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0774 - f1_macro: 0.9226\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0980 - f1_macro: 0.9020\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.1088 - f1_macro: 0.8912\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0984 - f1_macro: 0.9016\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0845 - f1_macro: 0.9155\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0807 - f1_macro: 0.9193\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.1461 - f1_macro: 0.8539\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.1285 - f1_macro: 0.8715\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.1127 - f1_macro: 0.8873\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0669 - f1_macro: 0.9331\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0880 - f1_macro: 0.9120\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0781 - f1_macro: 0.9219\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0787 - f1_macro: 0.9213\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0766 - f1_macro: 0.9234\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0813 - f1_macro: 0.9187\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0855 - f1_macro: 0.9145\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0825 - f1_macro: 0.9175\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0839 - f1_macro: 0.9161\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0776 - f1_macro: 0.9224\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0768 - f1_macro: 0.9232\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0740 - f1_macro: 0.9260\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0714 - f1_macro: 0.9286\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0740 - f1_macro: 0.9260\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0716 - f1_macro: 0.9284\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0676 - f1_macro: 0.9324\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 0.0906 - f1_macro: 0.9094\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1361 - f1_macro: 0.8639\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.1055 - f1_macro: 0.8945\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 0.0573 - f1_macro: 0.9427\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasClassifier(batch_size=64, loss=&lt;function f1_macro_loss at 0x7fad73464040&gt;,\n",
       "                metrics=[&lt;function f1_macro at 0x7fad7d245e50&gt;],\n",
       "                model=&lt;tensorflow.python.keras.engine.sequential.Sequential object at 0x7fac0f17ab80&gt;,\n",
       "                optimizer=&#x27;adam&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(batch_size=64, loss=&lt;function f1_macro_loss at 0x7fad73464040&gt;,\n",
       "                metrics=[&lt;function f1_macro at 0x7fad7d245e50&gt;],\n",
       "                model=&lt;tensorflow.python.keras.engine.sequential.Sequential object at 0x7fac0f17ab80&gt;,\n",
       "                optimizer=&#x27;adam&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasClassifier(batch_size=64, loss=<function f1_macro_loss at 0x7fad73464040>,\n",
       "                metrics=[<function f1_macro at 0x7fad7d245e50>],\n",
       "                model=<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fac0f17ab80>,\n",
       "                optimizer='adam')"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline.fit(X_train_seq, y_train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9996817e-01, 2.5228286e-05, 3.3673275e-06, 2.4877096e-07,\n",
       "        2.9375899e-06],\n",
       "       [1.4797124e-07, 6.3494889e-09, 1.0494427e-08, 8.7790859e-08,\n",
       "        9.9999976e-01],\n",
       "       [1.4175734e-08, 2.8724526e-10, 3.3442370e-11, 7.1463847e-07,\n",
       "        9.9999928e-01],\n",
       "       ...,\n",
       "       [6.7499947e-12, 7.9060734e-14, 2.4154062e-13, 1.6217501e-10,\n",
       "        1.0000000e+00],\n",
       "       [3.1674927e-12, 3.2129183e-14, 1.2527712e-13, 4.2963331e-11,\n",
       "        1.0000000e+00],\n",
       "       [9.3718411e-10, 5.0294692e-11, 2.8184314e-11, 1.2837758e-07,\n",
       "        9.9999988e-01]], dtype=float32)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.eye(len(np.unique(y_train_seq)))\n",
    "y_true = np.array([labels[yi - 1] for yi in y_train_seq])\n",
    "\n",
    "y_pred = model_pipeline.predict_proba(X_train_seq)\n",
    "# f1_macro(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>495</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  LABEL\n",
       "0      0      5\n",
       "1      1      4\n",
       "2      2      5\n",
       "3      3      1\n",
       "4      4      2\n",
       "..   ...    ...\n",
       "495  495      5\n",
       "496  496      5\n",
       "497  497      1\n",
       "498  498      1\n",
       "499  499      5\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_test_seq = pad_sequences(\n",
    "    X_test_transformed,\n",
    "    maxlen=max_len\n",
    ")\n",
    "\n",
    "predictions = model_pipeline.predict(X_test_seq)\n",
    "submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'LABEL': predictions})\n",
    "display(submission)\n",
    "submission.to_csv('test_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5: 322, 4: 65, 1: 94, 2: 6, 3: 13})"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midnight-ops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
