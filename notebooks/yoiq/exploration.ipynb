{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/02 19:02:50 INFO mlflow.tracking.fluent: Experiment with name 'ml-olympiad-tfugsurabaya-2024' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from lib.constants import EXPERIMENT_NAME, MLFLOW_URI\n",
    "from lib.dataset import load_train_data, load_test_data\n",
    "\n",
    "# Make sure to have the MLFlow server on before running this code.\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_URI)\n",
    "experiment = mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "train_data = load_train_data()\n",
    "test_data = load_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download necessary language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yoiqsram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yoiqsram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yoiqsram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yoiqsram/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK example usage on English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The waiter is not friendly, the person wears night guard glasses']\n",
      "['The', 'waiter', 'is', 'not', 'friendly', ',', 'the', 'person', 'wears', 'night', 'guard', 'glasses']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = train_data.REVIEW.iloc[0]\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waiter', 'friendly', ',', 'person', 'wears', 'night', 'guard', 'glasses']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waiter', 'friendli', ',', 'person', 'wear', 'night', 'guard', 'glass']\n",
      "['waiter', 'friendly', ',', 'person', 'wear', 'night', 'guard', 'glass']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmed_words = [porter.stem(word) for word in filtered_words]\n",
    "print(stemmed_words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('waiter', 'NN'), ('friendly', 'RB'), (',', ','), ('person', 'NN'), ('wear', 'JJ'), ('night', 'NN'), ('guard', 'NN'), ('glass', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tags = pos_tag(lemmatized_words)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of usage in Bahasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kakaknya', 'enak', 'bangetttt', ',', 'pelayanannya', 'cepet', 'tanggap', 'dan', 'yang', 'pertama', 'murah', 'senyum']\n",
      "['kakaknya', 'enak', 'bangetttt', ',', 'pelayanannya', 'cepet', 'tanggap', 'murah', 'senyum']\n",
      "['kakaknya', 'enak', 'bangetttt', ',', 'pelayanannya', 'cepet', 'tanggap', 'murah', 'senyum']\n",
      "['kakak', 'enak', 'bangetttt', '', 'layan', 'cepet', 'tanggap', 'murah', 'senyum']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "text = train_data.REVIEW.iloc[1]\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "# Filter stop words\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(filtered_words)\n",
    "\n",
    "# Stemming\n",
    "porter = PorterStemmer()\n",
    "stemmed_words = [porter.stem(word) for word in filtered_words]\n",
    "print(stemmed_words)\n",
    "\n",
    "# Lemmatization (using Sastrawi library)\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "stemmed_words_sastrawi = [stemmer.stem(word) for word in filtered_words]\n",
    "print(stemmed_words_sastrawi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>informal</th>\n",
       "      <th>formal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0kmh</td>\n",
       "      <td>0 kmh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1007mb</td>\n",
       "      <td>1007 mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1008mb</td>\n",
       "      <td>1008 mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1009mb</td>\n",
       "      <td>1009 mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100k</td>\n",
       "      <td>100 ribu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>yuuk</td>\n",
       "      <td>ayo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>yuukk</td>\n",
       "      <td>yuk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>yuuu</td>\n",
       "      <td>ayo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2621</th>\n",
       "      <td>yuuuk</td>\n",
       "      <td>ayo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>zonk</td>\n",
       "      <td>kosong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2623 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     informal    formal\n",
       "0        0kmh     0 kmh\n",
       "1      1007mb   1007 mb\n",
       "2      1008mb   1008 mb\n",
       "3      1009mb   1009 mb\n",
       "4        100k  100 ribu\n",
       "...       ...       ...\n",
       "2618     yuuk       ayo\n",
       "2619    yuukk       yuk\n",
       "2620     yuuu       ayo\n",
       "2621    yuuuk       ayo\n",
       "2622     zonk    kosong\n",
       "\n",
       "[2623 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lib.constants import PROJECT_DIR\n",
    "\n",
    "collex = pd.read_table(PROJECT_DIR / 'data' / 'indo-collex' / 'informal_to_formal_dictionary.tsv')\n",
    "collex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0kmh': '0 kmh',\n",
       " '1007mb': '1007 mb',\n",
       " '1008mb': '1008 mb',\n",
       " '1009mb': '1009 mb',\n",
       " '100k': '100 ribu',\n",
       " '1010mb': '1010 mb',\n",
       " '1011mb': '1011 mb',\n",
       " '1012mb': '1012 mb',\n",
       " '1017mb': '1017 mb',\n",
       " '1018mb': '1018 mb'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collex_dict = {row['informal']: row['formal'] for _, row in collex.iloc[:10].iterrows()}\n",
    "collex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29932\n"
     ]
    }
   ],
   "source": [
    "with open(PROJECT_DIR / 'data' / 'sastrawi' / 'kata-dasar.txt') as f:\n",
    "    words = set(f.read().splitlines())\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
